import { Fragment, useContext, useEffect, useRef, useState } from 'react';
import { useSettings } from 'context/SettingsContext';
import '../styles/hide-scrollbar.css';
import './ChatBot.css';
import { Welcome } from 'components';
import { BsMic } from 'react-icons/bs';
import { HiOutlineSpeakerWave, HiOutlineSpeakerXMark } from 'react-icons/hi2';
import { samplePhrases } from 'data';
import { useMessageContext } from 'context/MessageProvider';
import { BotStateContext } from 'context/BotState';
import SelectLang from 'components/SelectLang';
import SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition';
import axios from 'axios';
import PropTypes from 'prop-types';
import { ChachaCanvas } from './Bot';

const ChatBot = ({ setIsSpeaking }) => {
	const { settings } = useSettings();
    const particleCanvasRef = useRef(null);
	// Keep a ref to the latest ttsVoice so event handlers created on mount use
	// the current selection (avoids stale closures).
	const ttsVoiceRef = useRef(settings?.ttsVoice);
	useEffect(() => {
		ttsVoiceRef.current = settings?.ttsVoice;
	}, [settings?.ttsVoice]);
	// On mount, optionally trigger voice flow via custom event. We intentionally don't include
	// addMessage/getResponse/setIsSpeaking in deps to avoid re-wiring handlers repeatedly.
	useEffect(() => {
		const handler = (e) => {
				if (e.detail && e.detail.message === 'hello') {
					// Always activate voice mode (visuals) but only send the backend
					// request if the activation was user-initiated. This prevents an
					// automatic backend call on page refresh/autoplay.
					setAudio(true);
					const userInitiated = !!e.detail.userInitiated;
					if (!userInitiated) {
						// If not user-initiated, do not add a user message or call backend.
						// We optionally could show a local hint or simply activate voice UI.
						return;
					}
					// User initiated: send the greeting prompt to backend as before
					addMessage('user', 'hello');
					setState('waiting');
					getResponse({ prompt: 'hello' }, '/llama-chat').then(resp => {
						setState('idle');
						if (resp && resp.data && resp.data.result) {
							addMessage('assistant', resp.data.result);
							if (setIsSpeaking) setIsSpeaking(true);
							playTTS(resp.data.result);
						}
					});
				}
			};
		window.addEventListener('activate-chatbot-voice', handler);
		return () => window.removeEventListener('activate-chatbot-voice', handler);
	// eslint-disable-next-line react-hooks/exhaustive-deps
	}, []);

	// Canvas-based particle system (faster than DOM nodes). Respects settings.animationQuality
	useEffect(() => {
		const canvas = particleCanvasRef.current;
		if (!canvas) return;
		const ctx = canvas.getContext('2d');
		let width = 0;
		let height = 0;
		let dpr = Math.max(1, window.devicePixelRatio || 1);
		let animationId = null;
		let particles = [];
		let mouse = { x: -9999, y: -9999, lastMove: 0 };

		const quality = (settings && settings.animationQuality) ? settings.animationQuality : 'high';
	// Reduce default particle count to make the scene lighter by default.
	// 'low' remains low, 'off' keeps 0, and default/high uses a smaller count.
	const baseCount = quality === 'low' ? 20 : (quality === 'off' ? 0 : 60);

		function resize() {
			width = canvas.clientWidth || canvas.offsetWidth || window.innerWidth;
			height = canvas.clientHeight || canvas.offsetHeight || window.innerHeight;
			dpr = Math.max(1, window.devicePixelRatio || 1);
			canvas.width = Math.floor(width * dpr);
			canvas.height = Math.floor(height * dpr);
			canvas.style.width = width + 'px';
			canvas.style.height = height + 'px';
			ctx.setTransform(dpr, 0, 0, dpr, 0, 0);
		}

		function randomParticle() {
			return {
				x: Math.random() * width,
				y: height + (Math.random() * 40),
				vx: (Math.random() - 0.5) * 0.4,
				vy: - (0.2 + Math.random() * 0.8),
				size: Math.random() * 2.4 + 0.6,
				alpha: 0,
				life: 0,
				ttl: 4 + Math.random() * 8
			};
		}

		function ensureParticles() {
			const target = baseCount;
			while (particles.length < target) particles.push(randomParticle());
			if (particles.length > target) particles.length = target;
		}

		function draw() {
			ctx.clearRect(0,0,width,height);
			// subtle fade overlay to create trailing
			ctx.fillStyle = 'rgba(0,0,0,0)';
			ctx.fillRect(0,0,width,height);

			for (let i = 0; i < particles.length; i++) {
				const p = particles[i];
				p.x += p.vx;
				p.y += p.vy;
				p.life += 0.016;
				p.alpha = Math.min(1, p.life / 0.6) * (1 - (p.life / p.ttl));
				if (p.y < -20 || p.life > p.ttl) {
					particles[i] = randomParticle();
					continue;
				}
				ctx.beginPath();
				ctx.globalAlpha = Math.max(0, p.alpha * 0.9);
				const g = ctx.createRadialGradient(p.x, p.y, 0, p.x, p.y, p.size * 6);
				g.addColorStop(0, 'rgba(255,255,255,0.95)');
				g.addColorStop(0.2, 'rgba(255,255,255,0.5)');
				g.addColorStop(1, 'rgba(255,255,255,0)');
				ctx.fillStyle = g;
				ctx.arc(p.x, p.y, p.size * 2, 0, Math.PI * 2);
				ctx.fill();
				ctx.globalAlpha = 1;
			}

			// subtle parallax on spheres - handled via CSS transforms in mousemove
			animationId = requestAnimationFrame(draw);
		}

		function onMove(e) {
			// convert to canvas coords
			const rect = canvas.getBoundingClientRect();
			mouse.x = (e.clientX - rect.left) * (canvas.width / rect.width) / dpr;
			mouse.y = (e.clientY - rect.top) * (canvas.height / rect.height) / dpr;
			mouse.lastMove = Date.now();
			// burst small number of particles at mouse for interactivity
			const burst = (quality === 'low') ? 1 : 2;
			for (let i=0;i<burst;i++) {
				const p = randomParticle();
				p.x = (e.clientX - rect.left);
				p.y = (e.clientY - rect.top);
				p.vx = (Math.random()-0.5)*1.4;
				p.vy = - (0.6 + Math.random()*1.6);
				p.life = 0;
				particles.push(p);
			}
			// minor sphere parallax (kept as CSS transforms)
			const spheres = document.querySelectorAll('.chat-gradient-background .gradient-sphere');
			const moveX = (e.clientX / window.innerWidth - 0.5) * 8; // px
			const moveY = (e.clientY / window.innerHeight - 0.5) * 8;
			spheres.forEach(s => { try { s.style.transform = `translate(${moveX}px, ${moveY}px)`; } catch {} });
		}

		function start() {
			resize();
			ensureParticles();
			window.addEventListener('resize', resize);
			window.addEventListener('mousemove', onMove);
			draw();
		}

		function stop() {
			window.removeEventListener('resize', resize);
			window.removeEventListener('mousemove', onMove);
			if (animationId) cancelAnimationFrame(animationId);
			animationId = null;
			try { ctx.clearRect(0,0,canvas.width,canvas.height); } catch {}
		}

		start();

		return () => {
			stop();
			particles = [];
		};
	// Recreate when the animationQuality setting changes
	}, [settings?.animationQuality]);
	// Mic loudness state
	const [micLevel, setMicLevel] = useState(0);
	const audioContextRef = useRef(null);
	const analyserRef = useRef(null);
	const micStreamRef = useRef(null);
	// Play TTS audio for assistant responses
	async function playTTS(text) {
		try {
			// Allow using a dedicated TTS server via VITE_TTS_BASE_URL; fallback to backend API
			const prodApi = (import.meta?.env?.VITE_API_BASE_URL || window.location.origin.replace(/\/$/, ''));
			const ttsBaseEnv = import.meta?.env?.VITE_TTS_BASE_URL || '';
			const isDevHost = /:(5173|5174)$/i.test(window.location.origin || '') || !!import.meta?.env?.DEV;
			// Prefer dedicated TTS base; otherwise force /api in Vite dev so the proxy sends to Flask
			const apiBase = ttsBaseEnv || (isDevHost ? '/api' : prodApi);
			console.debug('playTTS base resolved', { apiBase, ttsBaseEnv, prodApi, origin: window.location.origin });
			// Pick a stable default if none selected (prefer male Hindi we installed)
			const voiceToSend = ttsVoiceRef.current || settings?.ttsVoice || 'hi_IN-rohan-medium';
			console.debug('playTTS sending voice=', voiceToSend);
			const postUrl = `${String(apiBase).replace(/\/$/, '')}/tts`;
			console.debug('playTTS POST', postUrl);
			let response = await fetch(postUrl, {
				method: 'POST',
				headers: { 'Content-Type': 'application/json' },
				// Use the ref to ensure the latest selected voice is used even when
				// this function is called from an event handler created earlier.
				body: JSON.stringify({
					text,
					voice: voiceToSend,
					rate: settings?.ttsRate,
					lang: settings?.ttsLang || 'en-IN',
				}),
			});
			if (!response.ok) {
				console.warn('TTS /tts returned non-OK', response.status);
				// Try a quick fallback endpoint if backend exposes it
				try {
					const fastUrl = `${String(apiBase).replace(/\/$/, '')}/fast-tts`;
					console.debug('playTTS fallback POST', fastUrl);
					response = await fetch(fastUrl, {
						method: 'POST',
						headers: { 'Content-Type': 'application/json' },
						body: JSON.stringify({ text, voice: voiceToSend, rate: settings?.ttsRate, lang: settings?.ttsLang || 'en-IN' }),
					});
				} catch (e) { /* no-op */ }
				if (!response.ok) return;
			}
			const blob = await response.blob();
			const audioUrl = URL.createObjectURL(blob);
			const audio = new Audio(audioUrl);
			audio.onended = () => { try { URL.revokeObjectURL(audioUrl); } catch {} };
			try {
				await audio.play();
				console.debug('TTS playback started');
			} catch (playErr) {
				console.warn('Browser blocked autoplay or playback failed; user gesture may be required', playErr);
			}
		} catch (err) {
			console.error('TTS error:', err);
		}
	}
	const [message, setMessage] = useState('');
	// audio=true: voice mode (3D mascot), audio=false: text mode (popup)
	const [audio, setAudio] = useState(true);
	const { botState, setBotState } = useContext(BotStateContext);
	const [isTyping, setIsTyping] = useState(false);
	const [state, setState] = useState('idle');
	const [lang, setLang] = useState('en-IN');
	const [streamingEnabled, setStreamingEnabled] = useState(true);
	const inputRef = useRef(null);

	const { chatHistory, addMessage, resetHistory, updateLastAssistant } = useMessageContext();

	const bottomRef = useRef(null);
	const containerRef = useRef(null);

	const { transcript, listening, resetTranscript, browserSupportsSpeechRecognition } = useSpeechRecognition();
	const [voiceError, setVoiceError] = useState(null);
	const [speechApiSupported, setSpeechApiSupported] = useState(false);
	const [mediaDevicesSupported, setMediaDevicesSupported] = useState(false);
	const [secureContextFlag, setSecureContextFlag] = useState(false);
	const [micPermission, setMicPermission] = useState('unknown');
	// Recording fallback state
	const [recState, setRecState] = useState('idle'); // idle|recording|ready
	const [recBlob, setRecBlob] = useState(null);
	const recChunksRef = useRef([]);
	const mediaRecorderRef = useRef(null);
	const [isOverlayVisible, setIsOverlayVisible] = useState(false);

	// Track voice talk time when listening is true
	useEffect(() => {
		const KEY = 'chatTalkSeconds';
		let intervalId = null;
		if (listening) {
			intervalId = setInterval(() => {
				try {
					const raw = localStorage.getItem(KEY);
					const val = raw ? parseInt(raw, 10) || 0 : 0;
					localStorage.setItem(KEY, String(val + 1));
				} catch { /* ignore */ }
			}, 1000);
		}
		return () => { if (intervalId) clearInterval(intervalId); };
	}, [listening]);

	// Diagnostics for voice features and permissions on mount
	useEffect(() => {
		try {
			setSpeechApiSupported(!!(window.SpeechRecognition || window.webkitSpeechRecognition));
		} catch { setSpeechApiSupported(false); }
		setMediaDevicesSupported(!!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia));
		setSecureContextFlag(!!(window.isSecureContext || window.location.protocol === 'https:' || /localhost|127\./.test(window.location.hostname)));
		try {
			if (navigator.permissions && navigator.permissions.query) {
				navigator.permissions.query({ name: 'microphone' }).then(res => {
					setMicPermission(res.state || 'unknown');
					res.onchange = () => setMicPermission(res.state || 'unknown');
				}).catch(() => setMicPermission('unknown'));
			}
		} catch { setMicPermission('unknown'); }
	}, []);

	useEffect(() => {
		// When chat updates, scroll the container (use instant scroll to avoid
		// the browser trying to scroll the whole page during a smooth animation).
		scrollToBottom({ smooth: false });
	}, [chatHistory, state]);

	const scrollToBottom = (opts = { smooth: false }) => {
	 	try {
	 		if (containerRef.current) {
	 			// Prefer setting scrollTop directly for more predictable behavior
	 			if (opts.smooth) {
	 				containerRef.current.scrollTo({ top: containerRef.current.scrollHeight, behavior: 'smooth' });
	 			} else {
	 				containerRef.current.scrollTop = containerRef.current.scrollHeight;
	 			}
	 			return;
	 		}
	 		if (bottomRef.current) {
	 			bottomRef.current.scrollIntoView({ behavior: opts.smooth ? 'smooth' : 'auto' });
	 		}
	 	} catch (e) { void e; }
	};

	useEffect(() => {
		setBotState(state);
		console.log(botState);
	// eslint-disable-next-line react-hooks/exhaustive-deps
	}, [state]);

	const focusInput = () => {
		try {
			if (inputRef.current) {
				if (typeof inputRef.current.focus === 'function') {
					try { inputRef.current.focus({ preventScroll: true }); }
					catch (e) { inputRef.current.focus(); }
				}
			}
		} catch (e) { void e; }
	};

	useEffect(() => {
		focusInput();
		// console.log(state, listening);
	}, [state]);

	// Resolve API base robustly across dev/prod/preview
	const isViteDev = !!(typeof import.meta !== 'undefined' && import.meta?.env?.DEV);
	const isViteDevPort = /:(5173|5174)$/i.test(window.location.origin || '');
	const resolvedApiBase = (isViteDev || isViteDevPort)
		? '/api'
		: (import.meta?.env?.VITE_API_BASE_URL || window.location.origin || '').replace(/\/$/, '');
	const baseURL = resolvedApiBase;

	const api = axios.create({
		baseURL: baseURL,
		headers: {
			'Content-Type': 'application/json'
		}
	});

	// const endpoint = 'http://localhost:5000/llama-chat';

	const getResponse = async (userdata, endpoint) => {
			try {
				let ageGroup = null;
				let name = undefined;
				try {
					const p = JSON.parse(localStorage.getItem('userProfile') || 'null');
					const rawAge = p?.age;
					name = p?.name;
					// Accept numbers or strings like "9" in stored profile
					let age = null;
					if (typeof rawAge === 'number') age = rawAge;
					else if (typeof rawAge === 'string' && /^\d+$/.test(rawAge.trim())) age = parseInt(rawAge.trim(), 10);
					if (typeof age === 'number' && !Number.isNaN(age)) {
						ageGroup = age <= 10 ? 'kid' : age < 16 ? 'teen' : 'adult';
					}
				} catch (e) { console.debug('userProfile parse error', e); }
				const userToken = localStorage.getItem('userToken');
				const payload = { ...userdata, ageGroup, name };
				// Provide recent conversation history for context (last 12 messages)
				try {
					payload.history = (chatHistory || []).slice(-12);
				} catch {}
				// Per-request speed preset: use 'fast' in voice mode by default (except for kids)
				try {
					const isKidLocal = (typeof ageGroup === 'string' ? ageGroup : payload.ageGroup) === 'kid';
					if (audio && !isKidLocal && !payload.speed) payload.speed = 'fast';
				} catch {}
				// In voice mode we prefer fast fallback, but let kids get full richer answers
				const isKid = payload.ageGroup === 'kid';
				if (audio && !payload.fallback && !isKid) {
					payload.fallback = true;
				}
				// Ensure we only send a sane Authorization header (avoid sending 'null' or empty)
				const safeToken = (userToken && userToken !== 'null') ? userToken : null;
				const headers = safeToken ? { 'Authorization': `Bearer ${safeToken}` } : {};

				// Debug: log baseURL, headers and payload to help diagnose preflight/latency
				try {
					console.debug('llama-chat request', { baseURL: api.defaults.baseURL, endpoint, headers, payload });
				} catch { /* ignore */ }

				const t0 = performance.now();
				const response = await api.post(endpoint, payload, { headers });
				const t1 = performance.now();
				const serverMs = (response?.data?.latency_ms ?? 0);
				const clientMs = Math.round(t1 - t0);
				console.log(`llama-chat: client ${clientMs}ms, server ${serverMs}ms, retrieved ${response?.data?.retrieved_count}`);
				if (clientMs > 3000 || serverMs > 3000) {
					console.warn('High latency detected for /llama-chat', { clientMs, serverMs, payloadPreview: { prompt: payload.prompt, ageGroup: payload.ageGroup } });
				}
			// console.log('response', response);
			setState('thinking');
			return response;
		} catch (err) {
			console.error(err);
		}
	};

	// Streamed response reader (NDJSON with lines like {"delta":"..."} and final {"done":true})
	const streamResponse = async (userdata) => {
		try {
			let ageGroup = null;
			let name = undefined;
			try {
				const p = JSON.parse(localStorage.getItem('userProfile') || 'null');
				const rawAge = p?.age; name = p?.name;
				let age = null;
				if (typeof rawAge === 'number') age = rawAge; else if (typeof rawAge === 'string' && /^\d+$/.test(rawAge.trim())) age = parseInt(rawAge.trim(), 10);
				if (typeof age === 'number' && !Number.isNaN(age)) ageGroup = age <= 10 ? 'kid' : age < 16 ? 'teen' : 'adult';
			} catch {}
			const payload = { ...userdata, ageGroup, name };
			try { payload.history = (chatHistory || []).slice(-12); } catch {}
			try {
				const isKidLocal = (typeof ageGroup === 'string' ? ageGroup : payload.ageGroup) === 'kid';
				if (audio && !isKidLocal && !payload.speed) payload.speed = 'fast'; else if (!payload.speed) payload.speed = 'balanced';
			} catch {}

			// Debug log for networking
			try { console.debug('llama-chat-stream request', { baseURL, url: `${baseURL}/llama-chat-stream`, payload }); } catch {}
			const streamUrl = `${String(baseURL).replace(/\/$/, '')}/llama-chat-stream`;
			const resp = await fetch(streamUrl, {
				method: 'POST',
				headers: { 'Content-Type': 'application/json' },
				body: JSON.stringify(payload)
			});
			if (!resp.ok || !resp.body) {
				console.warn('Stream endpoint failed; falling back', { status: resp.status });
				// Fallback to non-streaming path and handle UI updates here
				try {
					const fallback = await getResponse(userdata, '/llama-chat');
					setState('idle');
					if (fallback && fallback.data && fallback.data.result) {
						addMessage('assistant', fallback.data.result);
						try {
							if (audio) await playTTS(fallback.data.result);
						} catch {}
					}
					return;
				} catch (e) {
					console.error('fallback /llama-chat failed', e);
					setState('idle');
					return;
				}
			}
			setState('thinking');
			let created = false;
			// Collect the full assistant text locally so we can TTS it reliably
			let finalText = '';
			if (setIsSpeaking && audio) setIsSpeaking(true);
			const reader = resp.body.getReader();
			const decoder = new TextDecoder();
			let buffer = '';
			while (true) {
				const { value, done } = await reader.read();
				if (done) break;
				buffer += decoder.decode(value, { stream: true });
				let idx;
				while ((idx = buffer.indexOf('\n')) !== -1) {
					const line = buffer.slice(0, idx).trim();
					buffer = buffer.slice(idx + 1);
					if (!line) continue;
					try {
						const obj = JSON.parse(line);
						if (obj.delta) {
							if (!created) { addMessage('assistant', ''); created = true; }
							updateLastAssistant(String(obj.delta));
							finalText += String(obj.delta);
						} else if (obj.done) {
							// meta end; could use for telemetry if needed
						}
					} catch {}
				}
			}
			setState('idle');
			if (setIsSpeaking) setIsSpeaking(false);
			// After the streamed response completes, auto-play TTS if voice mode is on
			try {
				// Prefer the locally accumulated text to avoid stale state closures
				const textToSpeak = (finalText && finalText.trim().length > 0)
					? finalText
					: (() => { try { const last = (chatHistory || [])[Math.max(0, (chatHistory || []).length - 1)]; return (last && last.role === 'assistant') ? last.content : ''; } catch { return ''; } })();
				if (audio && textToSpeak) {
					await playTTS(textToSpeak);
				}
			} catch (e) { console.debug('stream TTS playback skipped', e); }
		} catch (e) {
			console.error('streamResponse error', e);
			setState('idle');
		}
	};

	// Start recording/recognition and show overlay UI
	const startRecording = async () => {
		setVoiceError(null);
		setIsOverlayVisible(true);
		// Try to start Web Speech API if available
		if (speechApiSupported && browserSupportsSpeechRecognition) {
			try {
				resetTranscript();
				// request mic first so permissions prompt appears
				const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
				audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
				micStreamRef.current = stream;
				const source = audioContextRef.current.createMediaStreamSource(stream);
				analyserRef.current = audioContextRef.current.createAnalyser();
				analyserRef.current.fftSize = 256;
				source.connect(analyserRef.current);
				// start recognition
				SpeechRecognition.startListening({ continuous: true, language: lang });
				// start level monitor loop
				const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
				const raf = () => {
					if (!analyserRef.current) return;
					analyserRef.current.getByteFrequencyData(dataArray);
					const values = dataArray.reduce((a, b) => a + b, 0);
					const average = values / dataArray.length;
					setMicLevel(average);
					requestAnimationFrame(raf);
				};
				requestAnimationFrame(raf);
			} catch (err) {
				console.error('startRecording (Speech API) error', err);
				setVoiceError('Could not start microphone: ' + (err?.message || err));
				setIsOverlayVisible(false);
			}
			return;
		}
		// Fallback: use MediaRecorder to capture audio and then send to server for STT
		if (mediaDevicesSupported) {
			try {
				const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
				audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
				micStreamRef.current = stream;
				const source = audioContextRef.current.createMediaStreamSource(stream);
				analyserRef.current = audioContextRef.current.createAnalyser();
				analyserRef.current.fftSize = 256;
				source.connect(analyserRef.current);
				const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
				const raf = () => {
					if (!analyserRef.current) return;
					analyserRef.current.getByteFrequencyData(dataArray);
					const values = dataArray.reduce((a, b) => a + b, 0);
					const average = values / dataArray.length;
					setMicLevel(average);
					requestAnimationFrame(raf);
				};
				requestAnimationFrame(raf);
				recChunksRef.current = [];
				const mr = new MediaRecorder(stream);
				mediaRecorderRef.current = mr;
				mr.ondataavailable = (e) => { if (e.data && e.data.size) recChunksRef.current.push(e.data); };
				mr.onstop = () => {
					const b = new Blob(recChunksRef.current, { type: 'audio/webm' });
					setRecBlob(b);
					setRecState('ready');
					try { stream.getTracks().forEach(t => t.stop()); } catch { }
				};
				mr.start();
				setRecState('recording');
			} catch (e) {
				console.error('Recorder start failed', e);
				setVoiceError('Recorder failed to start. Check mic permissions.');
				setIsOverlayVisible(false);
			}
			return;
		}
		setVoiceError('No available method to capture audio in this environment.');
		setIsOverlayVisible(false);
	};
	// Stop recording/recognition and process transcript
	const stopRecording = async () => {
		setIsOverlayVisible(false);
		// Stop SpeechRecognition if active
		try {
			if (speechApiSupported && browserSupportsSpeechRecognition && listening) {
				SpeechRecognition.stopListening();
				// wait a tick for final transcript to flush
				await new Promise(r => setTimeout(r, 250));
				const t = transcript?.trim();
				if (t && t.length) {
					setMessage(t);
				}
				resetTranscript();
			} else if (mediaRecorderRef.current && recState === 'recording') {
				try {
					mediaRecorderRef.current.stop();
				} catch { }
				// wait for onstop to set recBlob
				let attempts = 0;
				while (recState !== 'ready' && attempts < 40) {
					// wait for blob
					// eslint-disable-next-line no-await-in-loop
					await new Promise(r => setTimeout(r, 100));
					attempts++;
				}
				if (recBlob) {
					// We intentionally avoid using server-side STT here.
					// Prefer the browser SpeechRecognition (Web Speech API) which
					// populates `transcript`. If that's not available, inform the
					// user that local STT isn't supported instead of uploading audio.
					if (speechApiSupported && browserSupportsSpeechRecognition) {
						// Wait a moment for final transcript to settle (if any)
						await new Promise(r => setTimeout(r, 250));
						const t = transcript?.trim();
						if (t && t.length) {
							setMessage(t);
						} else {
							// No transcript even though API is present
							setVoiceError('No speech detected. Please try again or type your question.');
						}
					} else {
						setVoiceError('Local speech recognition not available in this browser. Please type your question or enable a browser that supports Web Speech API.');
					}
				} else {
					setVoiceError('Recording failed; no audio captured.');
				}
			}
		} catch (e) {
			console.error('stopRecording error', e);
			setVoiceError('Failed to process recording.');
		} finally {
			// cleanup audio graph
			if (micStreamRef.current) {
				try { micStreamRef.current.getTracks().forEach(t => t.stop()); } catch { }
				micStreamRef.current = null;
			}
			if (audioContextRef.current) {
				try { audioContextRef.current.close(); } catch { }
				audioContextRef.current = null;
			}
			analyserRef.current = null;
			mediaRecorderRef.current = null;
			setRecState('idle');
			setMicLevel(0);
		}
	};

	useEffect(() => {
		if (!isTyping && listening) {
			setMessage(transcript);
		}
	}, [transcript, listening, isTyping]);

	const handleInputChange = (e) => {
		setMessage(e.target.value);
		setIsTyping(true);
	};

	const handleInputBlur = () => {
		setIsTyping(false);
	};

	// Centralized send function used by button click and Enter key
	const sendMessage = async () => {
		try {
			SpeechRecognition.stopListening();
			resetTranscript();
			setIsTyping(false);
			if (message) {
				addMessage('user', message);
				setState('waiting');
				SpeechRecognition.abortListening();
				if (streamingEnabled) {
					await streamResponse({ prompt: message });
					setMessage('');
					resetTranscript();
				} else {
					let resp = await getResponse({ prompt: message }, '/llama-chat');
					setMessage('');
					resetTranscript();
					setState('idle');
					if (resp && resp.data && resp.data.result) {
						addMessage('assistant', resp.data.result);
						if (audio) {
							playTTS(resp.data.result);
							if (setIsSpeaking) setIsSpeaking(true);
						} else {
							if (setIsSpeaking) setIsSpeaking(false);
						}
					}
				}
			}
		} catch (e) { console.error('sendMessage error', e); }
	};

	const handleaudio = () => {
		setAudio(!audio);
		if (setIsSpeaking) setIsSpeaking(!audio); // update mascot speaking state
	};

	return (
	<main className='relative overflow-hidden bg-gray-900 bg-opacity-95 md:rounded-lg md:shadow-md p-4 pt-2 pb-3 w-full flex flex-col'>
		{/* Gradient animated background */}
		<div className='chat-gradient-background' aria-hidden>
			<div className='gradient-sphere sphere-1' />
			<div className='gradient-sphere sphere-2' />
			<div className='gradient-sphere sphere-3' />
			<div className='glow' />
			<div className='grid-overlay' />
			<div className='noise-overlay' />
			<canvas ref={particleCanvasRef} id='chat-particles-canvas' className='particles-canvas' aria-hidden />
		</div>
		<div className='main-chat-wrapper'>
			<div className='chat-shell'>
				<div className='chat-grid'>
					<aside className='left-rail'>
						<div id='chacha-3d-mount' className='model-holder chacha-3d-pill relative flex items-center justify-center'>
							<ChachaCanvas />
						</div>
					</aside>

					<section className='right-panel'>
						<div className='flex items-center justify-between mb-4'>
							<h1 className='text-2xl md:text-3xl font-extrabold text-white text-center flex-1'>
								Welcome to the Namami Gange Interactive Portal
								<span className='block text-blue-300 mt-2 text-lg md:text-xl font-semibold'>Chacha Chaudhary</span>
							</h1>
							<div className='flex items-center gap-2'>
								<button
									className='text-xs md:text-sm px-3 py-2 rounded-md bg-gray-800 text-gray-100 border border-gray-600 hover:bg-gray-700'
									onClick={() => { resetHistory(); setState('idle'); setMessage(''); }}
								>
									New chat
								</button>
								<button
									className='text-xs md:text-sm px-3 py-2 rounded-md bg-gray-800 text-gray-100 border border-gray-600 hover:bg-gray-700 flex items-center gap-2'
									onClick={handleaudio}
									title={audio ? 'Disable voice (no auto TTS)' : 'Enable voice (auto TTS)'}
								>
									{audio ? (<><HiOutlineSpeakerWave /><span className='hidden md:inline'>Voice on</span></>) : (<><HiOutlineSpeakerXMark /><span className='hidden md:inline'>Voice off</span></>)}
								</button>
								<label className='flex items-center gap-2 text-xs md:text-sm text-gray-300'>
									<input type='checkbox' checked={streamingEnabled} onChange={(e)=>setStreamingEnabled(e.target.checked)} />
									Stream replies
								</label>
							</div>
						</div>
						{/* Messages card */}
						<div ref={containerRef} className='messages-card hide-scrollbar flex-1 overflow-y-auto p-4 md:p-6'>
							<div className='flex flex-col space-y-4'>
								{chatHistory.length === 0 ? (
									<Fragment>
										{/* Personalized wave if name known */}
										{(() => { try { const p = JSON.parse(localStorage.getItem('userProfile')||'null'); if (p?.name) return (<p className='text-gray-300 text-sm mb-2'>Hi {p.name} 👋 — ask me anything about the Ganga, projects near you, or how to help.</p>); } catch {} return null; })()}
										<Welcome />
										<div className='grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-3'>
											{samplePhrases.map(phrase => (
												<button
													key={phrase}
													onClick={() => {
														addMessage('user', phrase);
														setTimeout(() => {
															setState('idle');
															addMessage('assistant', 'Wait, I am looking for your query!');
														}, 1100);
													}}
													className='bg-gray-100 border-gray-300 border-2 rounded-md p-2 text-sm'>
													{phrase}
												</button>
											))}
										</div>
									</Fragment>
								) : (
									chatHistory.map((chat, i) => (
										<div key={i} className={`flex ${chat.role === 'user' ? 'justify-end' : 'justify-start'} w-full`}>
											<div className={`rounded-2xl px-4 py-3 shadow-sm max-w-[70%] text-sm font-medium ${chat.role === 'user' ? 'bg-blue-700 text-white' : 'bg-gray-800 text-gray-100'} mb-3`}>
												{chat.content}
											</div>
										</div>
									))
								)}
								{/* Typing indicator bubble when assistant is thinking */}
								{(state === 'waiting' || state === 'thinking') && (
									<div className='flex justify-start w-full'>
										<div className='rounded-2xl px-4 py-3 shadow-sm max-w-[60%] text-sm font-medium bg-gray-800 text-gray-300 mb-3'>
											<span className='inline-flex items-center space-x-2'>
												<span className='w-2 h-2 bg-gray-400 rounded-full animate-bounce' style={{ animationDelay: '0ms' }} />
												<span className='w-2 h-2 bg-gray-400 rounded-full animate-bounce' style={{ animationDelay: '150ms' }} />
												<span className='w-2 h-2 bg-gray-400 rounded-full animate-bounce' style={{ animationDelay: '300ms' }} />
												<span className='ml-2'>Thinking…</span>
											</span>
										</div>
									</div>
								)}
							</div>
							<div ref={bottomRef} />
						</div>

						{/* Input row inside the right panel */}
						<div className='chat-box-input-field mt-3'>
						  <div className='chat-pill-container'>
							<div className='chat-pill' role='search'>
								<div className='chat-pill__left'>
									<div style={{ width: '120px' }}>
										<SelectLang setLang={setLang} />
									</div>
								</div>

								<div className='chat-pill__input-wrap'>
									{ (isOverlayVisible || listening || recState === 'recording') ? (
									  <div className='chat-pill__waveform' onClick={() => { /* focus or toggle */ }}>
										{Array.from({ length: 26 }).map((_, idx) => (
										  <VisualizerBar key={idx} idx={idx} analyserRef={analyserRef} listening={listening || recState === 'recording'} />
										))}
									  </div>
									) : (
									  <input
										type='text'
										ref={inputRef}
										className='chat-pill__input chat-input'
										placeholder={state === 'idle' ? 'Ask anything' : '...'}
										value={message}
										onChange={handleInputChange}
										onKeyDown={(e) => {
										  if (e.key === 'Enter' && !e.shiftKey) {
											e.preventDefault();
											void sendMessage();
										  }
										}}
										onBlur={handleInputBlur}
										disabled={state !== 'idle'}
									  />
									)}
								</div>

								<div className='chat-pill__actions'>
									<button
									  title={(isOverlayVisible || listening) ? 'Stop' : 'Start voice input'}
									  onClick={async () => {
										if (isOverlayVisible) await stopRecording(); else await startRecording();
									  }}
									  className={'chat-pill__mic ' + (isOverlayVisible || listening ? 'active' : '')}
									>
									  <BsMic />
									</button>

									<button
									  className='chat-pill__send'
									  onClick={() => { void sendMessage(); }}
									  disabled={!message || state !== 'idle'}
									  aria-label='Send'
									>
									  <span className='chat-pill__send-text'>Send</span>
									</button>
								</div>
							</div>
						  </div>
					</div>
					</section>
				</div>
			</div>
		</div>
	</main>
	);
};

export default ChatBot;

ChatBot.propTypes = {
	setIsSpeaking: PropTypes.func,
};

// Small visualizer bar component that reads the analyser node and scales height
function VisualizerBar({ idx, analyserRef, listening }) {
	const ref = useRef(null);
	useEffect(() => {
		let rafId = null;
		const el = ref.current;
		const update = () => {
			try {
				const analyser = analyserRef.current;
				if (!analyser || !el) {
					el.style.height = '4px';
					rafId = requestAnimationFrame(update);
					return;
				}
				const bins = analyser.frequencyBinCount;
				const arr = new Uint8Array(bins);
				analyser.getByteFrequencyData(arr);
				// pick a frequency band per bar, spread across the array
				const band = Math.floor((idx / 18) * bins);
				const v = arr[band] / 255; // 0..1
				const min = 4;
				const max = 48;
				const h = Math.round(min + v * (max - min));
				el.style.height = h + 'px';
				el.style.opacity = String(0.35 + v * 0.65);
			} catch (e) { /* ignore */ }
			rafId = requestAnimationFrame(update);
		};
		if (listening) update();
		return () => { if (rafId) cancelAnimationFrame(rafId); };
	}, [analyserRef, idx, listening]);
	return <div ref={ref} style={{ width: 6, height: 6, background: 'linear-gradient(180deg,#60a5fa,#0369a1)', borderRadius: 3 }} className='transform-gpu' />;
}
